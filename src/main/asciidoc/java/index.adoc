= Vert.x Kafka client

This component provides a Kafka client for reading and sending messages from/to an link:https://kafka.apache.org/[Apache Kafka] cluster.

From the consumer point of view, its API provides a bunch of methods for subscribing to a topic partition receiving
messages asynchronously or reading them as a stream (even with the possibility to pause the stream itself).
As producer, its API provides methods for sending message to a topic partition like writing on a stream.

WARNING: this module has the tech preview status, this means the API can change between versions.

== Using the Vert.x Kafka client

As component not yet officially released in the Vert.x stack, to use the Vert.x Kafka client current snapshot version,
add the following repository under the _repositories_ section and the following dependency to the _dependencies_ section
of your build descriptor:

* Maven (in your `pom.xml`):

[source,xml,subs="+attributes"]
----
<repository>
    <id>oss.sonatype.org-snapshot</id>
    <url>https://oss.sonatype.org/content/repositories/snapshots</url>
</repository>
----

[source,xml,subs="+attributes"]
----
<dependency>
    <groupId>io.vertx</groupId>
    <artifactId>vertx-kafka-client</artifactId>
    <version>3.4.0-SNAPSHOT</version>
</dependency>
----

* Gradle (in your `build.gradle` file):

[source,groovy,subs="+attributes"]
----
maven { url "https://oss.sonatype.org/content/repositories/snapshots" }
----

[source,groovy,subs="+attributes"]
----
compile io.vertx:vertx-kafka-client:3.4.0-SNAPSHOT
----

== Getting Started

=== Creating Kafka clients

The creation of both clients, consumer and producer, is quite similar and it's strictly related on how it works using
the native Kafka client library. They need to be configured with a bunch of properties as described in the official
Apache Kafka documentation, for the link:https://kafka.apache.org/documentation/#newconsumerconfigs[consumer] and
for the link:https://kafka.apache.org/documentation/#producerconfigs[producer].
In order to do that, a `link:../../apidocs/java/util/Properties.html[Properties]` instance can be filled with such properties passing it to one of the
static creation methods exposed by `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html[KafkaConsumer]` and
`link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html[KafkaProducer]` interfaces. Another way is filling a `link:../../apidocs/java/util/Map.html[Map]` instance
instead of the `link:../../apidocs/java/util/Properties.html[Properties]` one.
More advanced creation methods allow to specify the class type for the key and the value used for sending messages
or provided by received messages; this is a way for setting the key and value serializers/deserializers instead of
using the related properties for that.

[source,java]
----
Properties props = new Properties();
props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
props.put(ConsumerConfig.GROUP_ID_CONFIG, "my_group");
props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");

KafkaConsumer<String, String> consumer = KafkaConsumer.create(vertx, props);

// creating the producer using map and class types for key and value serializers/deserializers
Map<String, String> map = new HashMap<>();
map.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
map.put(ProducerConfig.ACKS_CONFIG, Integer.toString(1));

KafkaProducer<String, String> producer = KafkaProducer.create(vertx, map, String.class, String.class);
----

In the above example, a `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html[KafkaConsumer]` instance is created using a `link:../../apidocs/java/util/Properties.html[Properties]`
instance in order to specify the Kafka nodes list to connect (just one) and the deserializers to use for getting key
and value from each received message.
The `link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html[KafkaProducer]` instance is created in a different way using a `link:../../apidocs/java/util/Map.html[Map]`
instance for specifying Kafka nodes list to connect (just one) and the acknowledgment mode; the key and value
deserializers are specified as parameters in the
`link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html#create-io.vertx.core.Vertx-java.util.Map-java.lang.Class-java.lang.Class-[KafkaProducer.create]`
method.

=== Receiving messages from a topic joining a consumer group

In order to start receiving messages from Kafka topics, the consumer can use the
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#subscribe-java.util.Set-io.vertx.core.Handler-[subscribe]` method for subscribing
to a set of topics being part of a consumer group (specified by the properties on creation) and being notified when the operation
is completed. Before doing that, it's mandatory to register an handler for handling incoming messages using the
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#handler-io.vertx.core.Handler-[handler]` otherwise an
`link:../../apidocs/java/lang/IllegalStateException.html[IllegalStateException]` will be thrown.

Using the consumer group way, the Kafka cluster assigns partitions to the consumer taking into account other connected
consumers in the same consumer group, so that partitions can be spread across them. The Kafka cluster handles partitions re-balancing
when a consumer leaves the group (so assigned partitions are free to be assigned to other consumers) or a new consumer
joins the group (so it wants partitions to read from).
The `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html[KafkaConsumer]` interface provides a way for being notified
about what are the partitions revoked and assigned by the Kafka cluster specifying related handlers through the
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#partitionsRevokedHandler-io.vertx.core.Handler-[partitionsRevokedHandler]` and the
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#partitionsAssignedHandler-io.vertx.core.Handler-[partitionsAssignedHandler]`.

[source,java]
----
consumer.handler(record -> {
  System.out.println(record.key() + " " + record.value());
});

// registering handlers for assigned and revoked partitions
consumer.partitionsAssignedHandler(topicPartitions -> {

  System.out.println("Partitions assigned");
  topicPartitions.stream().forEach(topicPartition -> {
    System.out.println(topicPartition.getTopic() + " " + topicPartition.getPartition());
  });
});

consumer.partitionsRevokedHandler(topicPartitions -> {

  System.out.println("Partitions revoked");
  topicPartitions.stream().forEach(topicPartition -> {
    System.out.println(topicPartition.getTopic() + " " + topicPartition.getPartition());
  });
});

// subscribing to the topic
consumer.subscribe(Collections.singleton("test"), done -> {

  if (done.succeeded()) {
    System.out.println("Consumer subscribed");
  }
});
----

After joining a consumer group for receiving messages, a consumer can decide to leave the consumer group in order to
not get messages anymore. This is possible thanks to the `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#unsubscribe-io.vertx.core.Handler-[unsubscribe]`
method.

[source,java]
----
consumer.unsubscribe(done -> {

  if (done.succeeded()) {
    System.out.println("Consumer unsubscribed");
  }
});
----

=== Receiving messages from a topic requesting specific partitions

Other than being part of a consumer group for receiving messages from a topic, a consumer can ask for a specific
topic partition. The big difference is that without being part of a consumer group the overall application can't rely
on the re-balancing feature. The `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html[KafkaConsumer]` interface provides the
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#assign-java.util.Set-io.vertx.core.Handler-[assign]` method in order to
ask to be assigned specific partitions; using the `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#assignment-io.vertx.core.Handler-[assignment]`
method is also possible getting information about the current assigned partitions.

[source,java]
----
Set<TopicPartition> topicPartitions = new HashSet<>();
topicPartitions.add(new TopicPartition().setTopic("test").setPartition(0));

// registering the handler for incoming messages
consumer.handler(record -> {
  System.out.println(record.key() + " " + record.value());
});

// requesting to be assigned the specific partition
consumer.assign(topicPartitions, done -> {

  if (done.succeeded()) {
    System.out.println("Partition assigned");

    // requesting the assigned partitions
    consumer.assignment(done1 -> {

      if (done1.succeeded()) {

        done1.result().stream().forEach(topicPartition -> {
          System.out.println(topicPartition.getTopic() + " " + topicPartition.getPartition());
        });
      }
    });
  }
});
----

=== Getting topic partitions information

Both the `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html[KafkaConsumer]` and `link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html[KafkaProducer]`
interface provides the "partitionsFor" method for getting information about partitions in a specified topic.

[source,java]
----
consumer.listTopics(done -> {

  if (done.succeeded()) {

    done.result().forEach((topic, partitions) -> {

      System.out.println("topic = " + topic);
      System.out.println("partitions = " + partitions);
    });
  }
});

// asking partitions information about specific topic
consumer.partitionsFor("test", done -> {

  if (done.succeeded()) {

    done.result().stream().forEach(partitionInfo -> {
      System.out.println(partitionInfo);
    });
  }
});
----

The above example also shows that the `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html[KafkaConsumer]` interface provides one more
method for getting information about all available topics with related partitions.
This is the `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#listTopics-io.vertx.core.Handler-[listTopics]` method which is not
available in the `link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html[KafkaProducer]` interface.

=== Committing offset manually

In Apache Kafka, one of the main features is that the consumer is in charge to handle the offset of the last read message.
This is executed by the commit operation that can be executed automatically every time a bunch of messages are read
from a topic partition; in this case the "enable.auto.commit" configuration parameter needs to be set to "true" in
the properties bag for the consumer creation.
The other way is using the `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#commit-io.vertx.core.Handler-[commit]` method
in order to do that manually (it's useful for having an "at least once" delivery to be sure that the read messages
are processed before committing the offset).

[source,java]
----
consumer.commit(done -> {

  if (done.succeeded()) {
    System.out.println("Last read message offset committed");
  }
});
----

=== Sending messages to a topic

The `link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html[KafkaProducer]` interface provides the
`link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html#write-io.vertx.kafka.client.producer.KafkaProducerRecord-io.vertx.core.Handler-[write]`
method for sending messages (records) to a topic having the possibility to receive metadata about the messages sent like
the topic itself, the destination partition and the assigned offset. The simpler way is sending a message specifying
only the destination topic and the related value; in this case, without a key or a specific partition, the sender works
in a round robin way sending messages across all the partitions of the topic.

[source,java]
----
for (int i = 0; i < 5; i++) {

  // only topic and message value are specified, round robin on destination partitions
  KafkaProducerRecord<String, String> record =
    KafkaProducerRecord.create("test", "message_" + i);

  producer.write(record, recordMetadata -> {

    System.out.println("Message " + record.value() + " written on topic=" + recordMetadata.topic() +
      ", partition=" + recordMetadata.partition() +
      ", offset=" + recordMetadata.offset());

  });
}
----

In order to specify the destination partition for a message, it's possible to specify the partition identifier explicitly
or a key for the message.

[source,java]
----
for (int i = 0; i < 10; i++) {

  // a destination partition is specified
  KafkaProducerRecord<String, String> record =
    KafkaProducerRecord.create("test", null, "message_" + i, 0);

  producer.write(record, recordMetadata -> {

    System.out.println("Message " + record.value() + " written on topic=" + recordMetadata.topic() +
      ", partition=" + recordMetadata.partition() +
      ", offset=" + recordMetadata.offset());

  });
}
----

Using a key, the sender processes an hash on that in order to identify the destination partition; it
guarantees that all messages with the same key are sent to the same partition in order.

[source,java]
----
for (int i = 0; i < 10; i++) {

  // i.e. defining different keys for odd and even messages
  int key = i % 2;

  // a key is specified, all messages with same key will be sent to the same partition
  KafkaProducerRecord<String, String> record =
    KafkaProducerRecord.create("test", String.valueOf(key), "message_" + i);

  producer.write(record, recordMetadata -> {

    System.out.println("Message " + record.value() + " written on topic=" + recordMetadata.topic() +
      ", partition=" + recordMetadata.partition() +
      ", offset=" + recordMetadata.offset());

  });
}
----