= Vert.x Kafka client
:toc: left

This component provides a Kafka client for reading and sending messages from/to an link:https://kafka.apache.org/[Apache Kafka] cluster.

As consumer, the API provides methods for subscribing to a topic partition receiving
messages asynchronously or reading them as a stream (even with the possibility to pause/resume the stream).

As producer, the API provides methods for sending message to a topic partition like writing on a stream.

WARNING: this module has the tech preview status, this means the API can change between versions.

== Using the Vert.x Kafka client

As component not yet officially released in the Vert.x stack, to use the Vert.x Kafka client current snapshot version,
add the following repository under the _repositories_ section and the following dependency to the _dependencies_ section
of your build descriptor:

* Maven (in your `pom.xml`):

[source,xml,subs="+attributes"]
----
<repository>
    <id>oss.sonatype.org-snapshot</id>
    <url>https://oss.sonatype.org/content/repositories/snapshots</url>
</repository>
----

[source,xml,subs="+attributes"]
----
<dependency>
    <groupId>io.vertx</groupId>
    <artifactId>vertx-kafka-client</artifactId>
    <version>3.4.0-SNAPSHOT</version>
</dependency>
----

* Gradle (in your `build.gradle` file):

[source,groovy,subs="+attributes"]
----
maven { url "https://oss.sonatype.org/content/repositories/snapshots" }
----

[source,groovy,subs="+attributes"]
----
compile io.vertx:vertx-kafka-client:3.4.0-SNAPSHOT
----

== Creating Kafka clients

Creating consumers and sproducer is quite similar and on how it works using the native Kafka client library.

They need to be configured with a bunch of properties as described in the official
Apache Kafka documentation, for the link:https://kafka.apache.org/documentation/#newconsumerconfigs[consumer] and
for the link:https://kafka.apache.org/documentation/#producerconfigs[producer].

To achieve that, a `link:../../apidocs/java/util/Properties.html[Properties]` instance can be configured with such properties passing it to one of the
static creation methods exposed by `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html[KafkaConsumer]` and
`link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html[KafkaProducer]`

[source,groovy]
----

// creating the consumer using properties config
def config = new java.util.Properties()
config.put("bootstrap.servers", "localhost:9092")
config.put("key.deserializer", org.apache.kafka.common.serialization.StringDeserializer.class)
config.put("value.deserializer", org.apache.kafka.common.serialization.StringDeserializer.class)
config.put("group.id", "my_group")
config.put("auto.offset.reset", "earliest")
config.put("enable.auto.commit", "false")

// use consumer for interacting with Apache Kafka
def consumer = KafkaConsumer.create(vertx, config)

----

In the above example, a `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html[KafkaConsumer]` instance is created using
a `link:../../apidocs/java/util/Properties.html[Properties]` instance in order to specify the Kafka nodes list to connect (just one) and
the deserializers to use for getting key and value from each received message.

Another way is to use a `link:../../apidocs/java/util/Map.html[Map]` instance instead of the `link:../../apidocs/java/util/Properties.html[Properties]` which is available
only for the Java / Groovy / Kotlin

[source,groovy]
----

// creating the consumer using map config
def config = [:]
config["bootstrap.servers"] = "localhost:9092"
config["key.deserializer"] = "org.apache.kafka.common.serialization.StringDeserializer"
config["value.deserializer"] = "org.apache.kafka.common.serialization.StringSerializer"
config["group.id"] = "my_group"
config["auto.offset.reset"] = "earliest"
config["enable.auto.commit"] = "false"

// use consumer for interacting with Apache Kafka
def consumer = KafkaConsumer.create(vertx, config)

----

More advanced creation methods allow to specify the class type for the key and the value used for sending messages
or provided by received messages; this is a way for setting the key and value serializers/deserializers instead of
using the related properties for that

[source,groovy]
----

// creating the producer using map and class types for key and value serializers/deserializers
def map = [:]
map["bootstrap.servers"] = "localhost:9092"
map["acks"] = "1"

// use producer for interacting with Apache Kafka
def producer = KafkaProducer.create(vertx, map, java.lang.String.class, java.lang.String.class)

----

Here the `link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html[KafkaProducer]` instance is created in using a `link:../../apidocs/java/util/Map.html[Map]` for
specifying Kafka nodes list to connect (just one) and the acknowledgment mode; the key and value deserializers are
specified as parameters of `link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html#create-io.vertx.core.Vertx-java.util.Map-java.lang.Class-java.lang.Class-[KafkaProducer.create]`.

== Receiving messages from a topic joining a consumer group

In order to start receiving messages from Kafka topics, the consumer can use the
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#subscribe-java.util.Set-[subscribe]` method for
subscribing to a set of topics being part of a consumer group (specified by the properties on creation).

You need to register an handler for handling incoming messages using the
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#handler-io.vertx.core.Handler-[handler]`

[source,groovy]
----

// register the handler for incoming messages
consumer.handler({ record ->
  println("Processing key=${record.key()},value=${record.value()},partition=${record.partition()},offset=${record.offset()}")
})

// subscribe to several topics
def topics = new java.util.HashSet()
topics.add("topic1")
topics.add("topic2")
topics.add("topic3")
consumer.subscribe(topics)

// or just subscribe to a single topic
consumer.subscribe("a-single-topic")

----

An handler can also be passed during subscription to be aware of the subscription result and being notified when the operation
is completed.

[source,groovy]
----

// register the handler for incoming messages
consumer.handler({ record ->
  println("Processing key=${record.key()},value=${record.value()},partition=${record.partition()},offset=${record.offset()}")
})

// subscribe to several topics
def topics = new java.util.HashSet()
topics.add("topic1")
topics.add("topic2")
topics.add("topic3")
consumer.subscribe(topics, { ar ->
  if (ar.succeeded()) {
    println("subscribed")
  } else {
    println("Could not subscribe ${ar.cause().getMessage()}")
  }
})

// or just subscribe to a single topic
consumer.subscribe("a-single-topic", { ar ->
  if (ar.succeeded()) {
    println("subscribed")
  } else {
    println("Could not subscribe ${ar.cause().getMessage()}")
  }
})

----

Using the consumer group way, the Kafka cluster assigns partitions to the consumer taking into account other connected
consumers in the same consumer group, so that partitions can be spread across them.

The Kafka cluster handles partitions re-balancing when a consumer leaves the group (so assigned partitions are free
to be assigned to other consumers) or a new consumer joins the group (so it wants partitions to read from).

You can register handlers on a `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html[KafkaConsumer]` to be notified
of the partitions revocations and assignments by the Kafka cluster using
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#partitionsRevokedHandler-io.vertx.core.Handler-[partitionsRevokedHandler]` and
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#partitionsAssignedHandler-io.vertx.core.Handler-[partitionsAssignedHandler]`.

[source,groovy]
----

// register the handler for incoming messages
consumer.handler({ record ->
  println("Processing key=${record.key()},value=${record.value()},partition=${record.partition()},offset=${record.offset()}")
})

// registering handlers for assigned and revoked partitions
consumer.partitionsAssignedHandler({ topicPartitions ->

  println("Partitions assigned")
  topicPartitions.each { topicPartition ->
    println("${topicPartition.topic} ${topicPartition.partition}")
  }
})

consumer.partitionsRevokedHandler({ topicPartitions ->

  println("Partitions revoked")
  topicPartitions.each { topicPartition ->
    println("${topicPartition.topic} ${topicPartition.partition}")
  }
})

// subscribes to the topic
consumer.subscribe("test", { ar ->

  if (ar.succeeded()) {
    println("Consumer subscribed")
  }
})

----

After joining a consumer group for receiving messages, a consumer can decide to leave the consumer group in order to
not get messages anymore using `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#unsubscribe--[unsubscribe]`

[source,groovy]
----

// consumer is already member of a consumer group

// unsubscribing request
consumer.unsubscribe()

----

You can add an handler to be notified of the result

[source,groovy]
----

// consumer is already member of a consumer group

// unsubscribing request
consumer.unsubscribe({ ar ->

  if (ar.succeeded()) {
    println("Consumer unsubscribed")
  }
})

----

== Receiving messages from a topic requesting specific partitions

Besides being part of a consumer group for receiving messages from a topic, a consumer can ask for a specific
topic partition. When the consumer is not part part of a consumer group the overall application cannot
rely on the re-balancing feature.

You can use `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#assign-java.util.Set-io.vertx.core.Handler-[assign]`
in order to ask for specific partitions.

[source,groovy]
----

// register the handler for incoming messages
consumer.handler({ record ->
  println("key=${record.key()},value=${record.value()},partition=${record.partition()},offset=${record.offset()}")
})

//
def topicPartitions = new java.util.HashSet()
topicPartitions.add([
  topic:"test",
  partition:0
])

// requesting to be assigned the specific partition
consumer.assign(topicPartitions, { done ->

  if (done.succeeded()) {
    println("Partition assigned")

    // requesting the assigned partitions
    consumer.assignment({ done1 ->

      if (done1.succeeded()) {

        done1.result().each { topicPartition ->
          println("${topicPartition.topic} ${topicPartition.partition}")
        }
      }
    })
  }
})

----

Calling `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#assignment-io.vertx.core.Handler-[assignment]` provides
the list of the current assigned partitions.

== Getting topic partition information

You can call the `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#partitionsFor-java.lang.String-io.vertx.core.Handler-[partitionsFor]` to get information about
partitions for a specified topic

[source,groovy]
----

// asking partitions information about specific topic
consumer.partitionsFor("test", { ar ->

  if (ar.succeeded()) {

    ar.result().each { partitionInfo ->
      println(partitionInfo)
    }
  }
})

----

In addition `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#listTopics-io.vertx.core.Handler-[listTopics]` provides all available topics
with related partitions

[source,groovy]
----

// asking information about available topics and related partitions
consumer.listTopics({ ar ->

  if (ar.succeeded()) {

    def map = ar.result()
    map.each { topic, partitions ->
      println("topic = ${topic}")
      println("partitions = ${map[topic]}")
    }
  }
})

----

== Manual offset commit

In Apache Kafka the consumer is in charge to handle the offset of the last read message.

This is executed by the commit operation executed automatically every time a bunch of messages are read
from a topic partition. The configuration parameter `enable.auto.commit` must be set to `true` when the
consumer is created.

Manual offset commit, can be achieved with `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#commit-io.vertx.core.Handler-[commit]`.
It can be used to achieve _at least once_ delivery to be sure that the read messages are processed before committing
the offset.

[source,groovy]
----

// consumer is processing read messages

// committing offset of the last read message
consumer.commit({ ar ->

  if (ar.succeeded()) {
    println("Last read message offset committed")
  }
})

----

== Seeking in a topic partition

A great advantage of using Apache Kafka is that the messages are retained for a long period of time and the consumer can
seek inside a topic partition for re-reading all or part of the messages and then coming back to the end of
the partition. Using the `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#seek-io.vertx.kafka.client.common.TopicPartition-long-io.vertx.core.Handler-[seek]`
method it's possible to change the offset for starting to read at specific position. If the consumer needs to re-read the stream
from the beginning, there is the `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#seekToBeginning-java.util.Set-io.vertx.core.Handler-[seekToBeginning]`
method. Finally, in order to come back at the end of the partition, it's possible to use the
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#seekToEnd-java.util.Set-io.vertx.core.Handler-[seekToEnd]` method.

[source,groovy]
----

def topicPartition = [
  topic:"test",
  partition:0
]

// seeking to a specific offset
consumer.seek(topicPartition, 10, { done ->

  if (done.succeeded()) {
    println("Seeking done")
  }
})

// seeking at the beginning of the partition
consumer.seekToBeginning(java.util.Collections.singleton(topicPartition), { done ->

  if (done.succeeded()) {
    println("Seeking done")
  }
})

// seeking at the end of the partition
consumer.seekToEnd(java.util.Collections.singleton(topicPartition), { done ->

  if (done.succeeded()) {
    println("Seeking done")
  }
})

----

== Pausing and resuming the read on topic partitions

A consumer has the possibility to pause the read operation from a topic, in order to not receive other messages
(i.e. having more time to process the messages already read) and then resume the read for continuing to receive messages.
In order to do that, the `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html[KafkaConsumer]` interface provides the
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#pause-java.util.Set-io.vertx.core.Handler-[pause]` method and the
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#resume-java.util.Set-io.vertx.core.Handler-[resume]` method.

[source,groovy]
----

def topicPartitions = new java.util.HashSet()
topicPartitions.add([
  topic:"test",
  partition:0
])

// registering the handler for incoming messages
consumer.handler({ record ->
  println("key=${record.key()},value=${record.value()},partition=${record.partition()},offset=${record.offset()}")

  // i.e. pause/resume on partition 0, after reading message up to offset 5
  if ((record.partition() == 0) && (record.offset() == 5)) {

    // pausing read operation
    consumer.pause(topicPartitions, { done ->

      if (done.succeeded()) {

        println("Paused")
        // resuming read operation after a specific time
        vertx.setTimer(5000, { t ->

          // resuming read operation
          consumer.resume(topicPartitions, { done1 ->

            if (done1.succeeded()) {
              println("Resumed")
            }
          })

        })

      }
    })
  }
})

// subscribing to the topic
consumer.subscribe(java.util.Collections.singleton("test"), { done ->

  if (done.succeeded()) {
    println("Consumer subscribed")
  }
})

----

== Sending messages to a topic

The `link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html[KafkaProducer]` interface provides the
`link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html#write-io.vertx.kafka.client.producer.KafkaProducerRecord-io.vertx.core.Handler-[write]`
method for sending messages (records) to a topic having the possibility to receive metadata about the messages sent like
the topic itself, the destination partition and the assigned offset. The simpler way is sending a message specifying
only the destination topic and the related value; in this case, without a key or a specific partition, the sender works
in a round robin way sending messages across all the partitions of the topic.

[source,groovy]
----

(0..<5).each { i ->

  // only topic and message value are specified, round robin on destination partitions
  def record = KafkaProducerRecord.create("test", "message_${i}")

  producer.write(record, { done ->

    if (done.succeeded()) {

      def recordMetadata = done.result()
      println("Message ${record.value()} written on topic=${recordMetadata.topic}, partition=${recordMetadata.partition}, offset=${recordMetadata.offset}")
    }

  })
}


----

In order to specify the destination partition for a message, it's possible to specify the partition identifier explicitly
or a key for the message.

[source,groovy]
----

(0..<10).each { i ->

  // a destination partition is specified
  def record = KafkaProducerRecord.create("test", null, "message_${i}", 0)

  producer.write(record, { done ->

    if (done.succeeded()) {

      def recordMetadata = done.result()
      println("Message ${record.value()} written on topic=${recordMetadata.topic}, partition=${recordMetadata.partition}, offset=${recordMetadata.offset}")
    }

  })
}


----

Using a key, the sender processes an hash on that in order to identify the destination partition; it
guarantees that all messages with the same key are sent to the same partition in order.

[source,groovy]
----

(0..<10).each { i ->

  // i.e. defining different keys for odd and even messages
  def key = i % 2

  // a key is specified, all messages with same key will be sent to the same partition
  def record = KafkaProducerRecord.create("test", java.lang.String.valueOf(key), "message_${i}")

  producer.write(record, { done ->

    if (done.succeeded()) {

      def recordMetadata = done.result()
      println("Message ${record.value()} written on topic=${record.value()}, partition=${record.value()}, offset=${recordMetadata.offset}")
    }

  })
}


----

== Getting topic partition information

You can call the `link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html#partitionsFor-java.lang.String-io.vertx.core.Handler-[partitionsFor]` to get information about
partitions for a specified topic:

[source,groovy]
----

// asking partitions information about specific topic
producer.partitionsFor("test", { ar ->

  if (ar.succeeded()) {

    ar.result().each { partitionInfo ->
      println(partitionInfo)
    }
  }
})

----

== Handling exceptions and errors

In order to handle potential errors and exceptions during the communication between a Kafka client (consumer or producer)
and the Kafka cluster, both `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html[KafkaConsumer]` and `link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html[KafkaProducer]`
interface provide the "exceptionHandler" method for setting an handler called when an error happens (i.e. timeout).

[source,groovy]
----

// setting handler for errors
consumer.exceptionHandler({ e ->
  println("Error = ${e.getMessage()}")
})

----

== Stream implementation and native Kafka objects

Other than the polyglot version of the Kafka consumer and producer, this component provides a stream oriented
implementation which handles native Kafka objects (and not the related Vert.x counterparts).
The available interfaces are `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaReadStream.html[KafkaReadStream]` for reading topic partitions and
`link:../../apidocs/io/vertx/kafka/client/producer/KafkaWriteStream.html[KafkaWriteStream]` for writing to topics. The extends the interfaces provided
by Vert.x for handling stream so the `link:../../apidocs/io/vertx/core/streams/ReadStream.html[ReadStream]` and `link:../../apidocs/io/vertx/core/streams/WriteStream.html[WriteStream]`
where the handled classes are the native ones from the Kafka client libraries like the
`link:../../apidocs/org/apache/kafka/clients/consumer/ConsumerRecord.html[ConsumerRecord]` and the `link:../../apidocs/org/apache/kafka/clients/producer/ProducerRecord.html[ProducerRecord]`.
The way to interact with the above streams is quite similar to the polyglot version.